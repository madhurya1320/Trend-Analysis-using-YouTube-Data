{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fce1260-9fd3-48fc-b72b-ce0f9b1bb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script fetches and writes YouTube trending video data into separate CSV files for each specified country.\n",
    "# It ensures that data is only fetched once per day per country.\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize the YouTube API client with valid API key\n",
    "api_key = 'Insert Your API Key Here'\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# List of country codes for which to fetch YouTube trending data\n",
    "countries = [\"CA\", \"FR\", \"GB\", \"IN\", \"JP\", \"MX\", \"RU\", \"US\"]\n",
    "\n",
    "# Create a directory named 'youtube_trending_data' to store CSV files\n",
    "data_directory = 'input'\n",
    "os.makedirs(data_directory, exist_ok=True)\n",
    "\n",
    "# Define the headers for the CSV files\n",
    "column_headers = [\"video_id\", \"title\", \"publishedAt\", \"channelId\", \"channelTitle\",\n",
    "                  \"categoryId\", \"trending_date\", \"tags\", \"view_count\", \"likes\",\n",
    "                  \"comment_count\", \"thumbnail_link\", \"comments_disabled\",\n",
    "                  \"ratings_disabled\", \"description\"]\n",
    "\n",
    "# Loop through each country code to fetch and save the trending video data\n",
    "for country_code in countries:\n",
    "    # Construct the file path for the CSV file for each country\n",
    "    file_path = os.path.join(data_directory, f'{country_code}_youtube_trending_data.csv')\n",
    "    \n",
    "    # Flag to determine whether new data needs to be fetched\n",
    "    fetch_data = True\n",
    "    today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Check if the file already exists and if the last entry is from today\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            last_record = None\n",
    "            # Iterate through the file to find the last non-empty row\n",
    "            for record in csv_reader:\n",
    "                if record:\n",
    "                    last_record = record\n",
    "\n",
    "            # Check if the last record's date matches today's date\n",
    "            if last_record and last_record[6].split('T')[0] == today_date:\n",
    "                fetch_data = False  # Skip fetching data if already done today\n",
    "\n",
    "    # Fetch and write data to the CSV file if not done already for today\n",
    "    if fetch_data:\n",
    "        with open(file_path, 'a', newline='', encoding='utf-8') as file:\n",
    "            csv_writer = csv.writer(file)\n",
    "\n",
    "            # Write the column headers if the file is new\n",
    "            if not os.path.getsize(file_path):\n",
    "                csv_writer.writerow(column_headers)\n",
    "\n",
    "            # Setup for YouTube API request pagination\n",
    "            max_results_per_request = 50  # Maximum results YouTube API allows per request\n",
    "            total_videos_to_fetch = 200   # Total number of videos to fetch\n",
    "            processed_videos = 0         # Counter for the number of videos processed\n",
    "            page_token = None            # Token for handling pagination\n",
    "\n",
    "            # Continue fetching data until the desired number of videos is reached\n",
    "            while processed_videos < total_videos_to_fetch:\n",
    "                # Make a request to the YouTube API for trending videos\n",
    "                video_request = youtube.videos().list(\n",
    "                    part=\"snippet,contentDetails,statistics\",\n",
    "                    chart='mostPopular',\n",
    "                    regionCode=country_code,\n",
    "                    maxResults=max_results_per_request,\n",
    "                    pageToken=page_token\n",
    "                )\n",
    "                video_response = video_request.execute()\n",
    "\n",
    "                # Process each video in the response\n",
    "                for video in video_response.get('items', []):\n",
    "                    snippet = video['snippet']\n",
    "                    statistics = video['statistics']\n",
    "\n",
    "                    # Extract and organize the video details\n",
    "                    video_details = [\n",
    "                        video['id'],\n",
    "                        snippet['title'],\n",
    "                        snippet['publishedAt'],\n",
    "                        snippet['channelId'],\n",
    "                        snippet['channelTitle'],\n",
    "                        snippet['categoryId'],\n",
    "                        today_date,  # Use today's date as the trending date\n",
    "                        ','.join(snippet.get('tags', [])),\n",
    "                        statistics.get('viewCount', 0),\n",
    "                        statistics.get('likeCount', 0),\n",
    "                        statistics.get('commentCount', 0),\n",
    "                        snippet['thumbnails']['default']['url'],\n",
    "                        snippet.get('comments_disabled', False),\n",
    "                        snippet.get('ratings_disabled', False),\n",
    "                        snippet['description']\n",
    "                    ]\n",
    "\n",
    "                    # Write the video details to the CSV file\n",
    "                    csv_writer.writerow(video_details)\n",
    "\n",
    "                    # Increment the processed videos counter\n",
    "                    processed_videos += 1\n",
    "                    if processed_videos >= total_videos_to_fetch:\n",
    "                        break\n",
    "\n",
    "                # Update the page token for the next batch of videos\n",
    "                page_token = video_response.get('nextPageToken')\n",
    "                if not page_token:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f86af1-ae0e-47e7-827a-6a5a429304f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69fb759c-4109-4dc5-9a4e-a434b27d74a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YouTube video categories have been saved to 'US_category_id.json'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552398e-bb4d-492a-8f98-064273e53d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
